{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Image Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our reverse image search, we used the ResNet18 model achieved from Task 1 along with the Approximate Nearest Neighbors Oh Yeah library by Spotify to find the nearest neighbours of each image (more specifically, the nearest neighbour of the image's feature vector). The nearest neighbours are saved into a tree data structure for faster retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms \n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "data_csv = pd.read_csv('final_image_data_path.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_path, model, image_size=IMAGE_SIZE):\n",
    "# Create a new model that outputs the desired feature layer\n",
    "    print(img_path) # For debugging\n",
    "\n",
    "    feature_model = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "    feature_model.eval()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    resize = transform_test(image).to(device)\n",
    "    \n",
    "    print(resize.shape)\n",
    "\n",
    "    # Extract features from the image\n",
    "    features = feature_model(resize.unsqueeze(0))\n",
    "\n",
    "    # Normalize the features\n",
    "    flattened_features = features.flatten()\n",
    "    normalised_features = flattened_features / torch.norm(flattened_features)\n",
    "\n",
    "    # Convert to NumPy Array\n",
    "    normalised_features = normalised_features.cpu().detach().numpy()\n",
    "\n",
    "    return normalised_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "base_model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image paths from the csv and put into a list\n",
    "image_paths = data_csv['image_path'].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of all images in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from each image from the path list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "\n",
    "for i in tqdm_notebook(range(len(image_paths))):\n",
    "    feature_list.append(extract_features(image_paths[i], base_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pickles to avoid having to recompute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dir = os.path.join(os.getcwd(), \"reverse_image_pickles\", \"final_vgg\")\n",
    "\n",
    "if not os.path.exists(pickle_dir):\n",
    "    os.makedirs(pickle_dir)\n",
    "\n",
    "pickle.dump(feature_list, open(os.path.join(pickle_dir, 'features.pickle'),'wb'))\n",
    "pickle.dump(image_paths, open(os.path.join(pickle_dir, 'image_paths.pickle'),'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features vectors and image paths from the pickle files (this is mainly for local testing, as this allows us to avoid having to recompute the features every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dir = os.path.join(os.getcwd(), \"reverse_image_pickles\", \"final_vgg\")\n",
    "print(pickle_dir)\n",
    "#Getting filenames and features from pickle files\n",
    "with open(os.path.join(pickle_dir, 'features.pickle'), 'rb') as f:\n",
    "    feature_list = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(pickle_dir, 'image_paths.pickle'), 'rb') as f:\n",
    "    filenames = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the label from the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for files in filenames:\n",
    "    label = files.split('\\\\')[-2]\n",
    "    labels.append(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the image path, image representation and label. This dataframe will be used for to store the location of the image, the image representation and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'img_id':filenames, 'img_repr': feature_list, 'label': labels})\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is saved as a pickle file for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(os.path.join(pickle_dir, 'df.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def convert_to_jpg_arr(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        with io.BytesIO() as output:\n",
    "            image.convert('RGB').save(output, format='JPEG')\n",
    "            output.seek(0)\n",
    "            return np.asarray(Image.open(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is where we can specify a new image to test the reverse image search. The feature of this new image is extracted and compared to the features of the images in the dataset. The nearest neighbours are then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new image to the data\n",
    "img_location = \"IMG_0634.PNG\"\n",
    "\n",
    "# Extract the features of the new image\n",
    "new_features = extract_features(img_location, base_model)\n",
    "\n",
    "# Add new_features to the feature_list2\n",
    "feature_list2 = feature_list.copy()\n",
    "feature_list2.append(new_features)\n",
    "\n",
    "# Clone df and add the new image to it\n",
    "df2 = df.copy()\n",
    "\n",
    "plt.imshow(convert_to_jpg_arr(img_location))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a copy of the feature_list and the dataframe, as during development if we only use the original ones, new images will be added into the feature_list and dataframe every time we run the code. This is not ideal as we want to keep the feature_list and dataframe constant for testing and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [img_location, new_features, '']\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2['img_repr'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we produce the nearest neighbours tree using AnnoyIndex from the annoy library. This tree allows us to retrieve the closest images to the query image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "\n",
    "f = len(df2['img_repr'][0])\n",
    "t = AnnoyIndex(f, metric='euclidean')\n",
    "\n",
    "for i in tqdm(range(len(feature_list2))):\n",
    "    t.add_item(i, feature_list2[i])\n",
    "    \n",
    "_ = t.build(150, n_jobs=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function we use to get a new dataframe storing the 10 images that have closest features to the inputted one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images_annoy(img_index):\n",
    "    base_img_id, base_vector, base_label  = df2.iloc[img_index, [0, 1, 2]]\n",
    "    similar_img_ids = t.get_nns_by_item(img_index, 11)\n",
    "    return base_img_id, base_label, df2.iloc[similar_img_ids[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image, base_label, similar_images_df = get_similar_images_annoy(len(df2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another show images function but this time will be in 3 rows with 4 images each\n",
    "def show_images(new_img_path):\n",
    "    plt.figure(figsize = (20,20))\n",
    "    \n",
    "    plt.subplot(3,4,1)\n",
    "    image = convert_to_jpg_arr(new_img_path)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Base Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    for i in range(len(similar_images_df)):\n",
    "        path = os.path.join(similar_images_df.iloc[i,0])\n",
    "        image = mpimg.imread(path)\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Similar Image ' + similar_images_df.iloc[i,2])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(img_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
